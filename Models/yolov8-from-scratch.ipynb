{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12725217,"sourceType":"datasetVersion","datasetId":8042921},{"sourceId":12725875,"sourceType":"datasetVersion","datasetId":8038017}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import\nimport math, os, random, cv2, numpy, torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:23.352311Z","iopub.execute_input":"2025-08-10T14:08:23.352747Z","iopub.status.idle":"2025-08-10T14:08:29.925923Z","shell.execute_reply.started":"2025-08-10T14:08:23.352725Z","shell.execute_reply":"2025-08-10T14:08:29.925252Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### (a) Conv ","metadata":{}},{"cell_type":"code","source":"class Conv(nn.Module):\n    def __init__(self,in_channels, out_channels,kernel_size=3,stride=1,padding=1,groups=1,activation=True):\n        super().__init__()\n        self.conv=nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False,groups=groups)\n        self.bn=nn.BatchNorm2d(out_channels,eps=0.001,momentum=0.03)\n        self.act=nn.SiLU(inplace=True) if activation else nn.Identity()\n\n    def forward(self,x):\n        return self.act(self.bn(self.conv(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:29.932076Z","iopub.execute_input":"2025-08-10T14:08:29.932292Z","iopub.status.idle":"2025-08-10T14:08:29.969321Z","shell.execute_reply.started":"2025-08-10T14:08:29.932270Z","shell.execute_reply":"2025-08-10T14:08:29.968581Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### (b) C2f","metadata":{}},{"cell_type":"code","source":"# 2.1 Bottleneck: staack of 2 COnv with shortcut connnection (True/False)\nclass Bottleneck(nn.Module):\n    def __init__(self,in_channels,out_channels,shortcut=True):\n        super().__init__()\n        self.conv1=Conv(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv2=Conv(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.shortcut=shortcut\n\n    def forward(self,x):\n        x_in=x # for residual connection\n        x=self.conv1(x)\n        x=self.conv2(x)\n        if self.shortcut:\n            x=x+x_in\n        return x\n    \n# 2.2 C2f: Conv + bottleneck*N+ Conv\nclass C2f(nn.Module):\n    def __init__(self,in_channels,out_channels, num_bottlenecks,shortcut=True):\n        super().__init__()\n        \n        self.mid_channels=out_channels//2\n        self.num_bottlenecks=num_bottlenecks\n\n        self.conv1=Conv(in_channels,out_channels,kernel_size=1,stride=1,padding=0)\n        \n        # sequence of bottleneck layers\n        self.m=nn.ModuleList([Bottleneck(self.mid_channels,self.mid_channels) for _ in range(num_bottlenecks)])\n\n        self.conv2=Conv((num_bottlenecks+2)*out_channels//2,out_channels,kernel_size=1,stride=1,padding=0)\n    \n    def forward(self,x):\n        x=self.conv1(x)\n\n        # split x along channel dimension\n        x1,x2=x[:,:x.shape[1]//2,:,:], x[:,x.shape[1]//2:,:,:]\n        \n        # list of outputs\n        outputs=[x1,x2] # x1 is fed through the bottlenecks\n\n        for i in range(self.num_bottlenecks):\n            x1=self.m[i](x1)    # [bs,0.5c_out,w,h]\n            outputs.insert(0,x1)\n\n        outputs=torch.cat(outputs,dim=1) # [bs,0.5c_out(num_bottlenecks+2),w,h]\n        out=self.conv2(outputs)\n\n        return out\n         \n# sanity check\nc2f=C2f(in_channels=64,out_channels=128,num_bottlenecks=2)\nprint(f\"{sum(p.numel() for p in c2f.parameters())/1e6} million parameters\")\n\ndummy_input=torch.rand((1,64,244,244))\ndummy_input=c2f(dummy_input)\nprint(\"Output shape: \", dummy_input.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:29.971470Z","iopub.execute_input":"2025-08-10T14:08:29.971717Z","iopub.status.idle":"2025-08-10T14:08:30.720948Z","shell.execute_reply.started":"2025-08-10T14:08:29.971693Z","shell.execute_reply":"2025-08-10T14:08:30.720055Z"}},"outputs":[{"name":"stdout","text":"0.18944 million parameters\nOutput shape:  torch.Size([1, 128, 244, 244])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### (c) SPPF","metadata":{}},{"cell_type":"code","source":"class SPPF(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=5):\n        #kernel_size= size of maxpool\n        super().__init__()\n        hidden_channels=in_channels//2\n        self.conv1=Conv(in_channels,hidden_channels,kernel_size=1,stride=1,padding=0)\n        # concatenate outputs of maxpool and feed to conv2\n        self.conv2=Conv(4*hidden_channels,out_channels,kernel_size=1,stride=1,padding=0)\n\n        # maxpool is applied at 3 different sacles\n        self.m=nn.MaxPool2d(kernel_size=kernel_size,stride=1,padding=kernel_size//2,dilation=1,ceil_mode=False)\n    \n    def forward(self,x):\n        x=self.conv1(x)\n\n        # apply maxpooling at diffent scales\n        y1=self.m(x)\n        y2=self.m(y1)\n        y3=self.m(y2)\n\n        # concantenate \n        y=torch.cat([x,y1,y2,y3],dim=1)\n\n        # final conv\n        y=self.conv2(y)\n\n        return y\n\n# sanity check\nsppf=SPPF(in_channels=128,out_channels=512)\nprint(f\"{sum(p.numel() for p in sppf.parameters())/1e6} million parameters\")\n\ndummy_input=sppf(dummy_input)\nprint(\"Output shape: \", dummy_input.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:30.721805Z","iopub.execute_input":"2025-08-10T14:08:30.722130Z","iopub.status.idle":"2025-08-10T14:08:31.635520Z","shell.execute_reply.started":"2025-08-10T14:08:30.722100Z","shell.execute_reply":"2025-08-10T14:08:31.634808Z"}},"outputs":[{"name":"stdout","text":"0.140416 million parameters\nOutput shape:  torch.Size([1, 512, 244, 244])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Putting things together","metadata":{}},{"cell_type":"code","source":"# backbone = DarkNet53\n\n# return d,w,r based on version\ndef yolo_params(version):\n    if version=='n':\n        return 1/3,1/4,2.0\n    elif version=='s':\n        return 1/3,1/2,2.0\n    elif version=='m':\n        return 2/3,3/4,1.5\n    elif version=='l':\n        return 1.0,1.0,1.0\n    elif version=='x':\n        return 1.0,1.25,1.0\n    \nclass Backbone(nn.Module):\n    def __init__(self,version,in_channels=3,shortcut=True):\n        super().__init__()\n        d,w,r=yolo_params(version)\n\n        # conv layers\n        self.conv_0=Conv(in_channels,int(64*w),kernel_size=3,stride=2,padding=1)\n        self.conv_1=Conv(int(64*w),int(128*w),kernel_size=3,stride=2,padding=1)\n        self.conv_3=Conv(int(128*w),int(256*w),kernel_size=3,stride=2,padding=1)\n        self.conv_5=Conv(int(256*w),int(512*w),kernel_size=3,stride=2,padding=1)\n        self.conv_7=Conv(int(512*w),int(512*w*r),kernel_size=3,stride=2,padding=1)\n\n        # c2f layers\n        self.c2f_2=C2f(int(128*w),int(128*w),num_bottlenecks=int(3*d),shortcut=True)\n        self.c2f_4=C2f(int(256*w),int(256*w),num_bottlenecks=int(6*d),shortcut=True)\n        self.c2f_6=C2f(int(512*w),int(512*w),num_bottlenecks=int(6*d),shortcut=True)\n        self.c2f_8=C2f(int(512*w*r),int(512*w*r),num_bottlenecks=int(3*d),shortcut=True)\n\n        # sppf\n        self.sppf=SPPF(int(512*w*r),int(512*w*r))\n    \n    def forward(self,x):\n        x=self.conv_0(x)\n        x=self.conv_1(x)\n\n        x=self.c2f_2(x)\n\n        x=self.conv_3(x)\n\n        out1=self.c2f_4(x) # keep for output\n\n        x=self.conv_5(out1)\n\n        out2=self.c2f_6(x) # keep for output\n\n        x=self.conv_7(out2)\n        x=self.c2f_8(x)\n        out3=self.sppf(x)\n\n        return out1,out2,out3\n\nprint(\"----Nano model -----\")\nbackbone_n=Backbone(version='n')\nprint(f\"{sum(p.numel() for p in backbone_n.parameters())/1e6} million parameters\")\n\nprint(\"----Small model -----\")\nbackbone_s=Backbone(version='s')\nprint(f\"{sum(p.numel() for p in backbone_s.parameters())/1e6} million parameters\")\n        \n\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:31.636269Z","iopub.execute_input":"2025-08-10T14:08:31.636472Z","iopub.status.idle":"2025-08-10T14:08:31.708948Z","shell.execute_reply.started":"2025-08-10T14:08:31.636455Z","shell.execute_reply":"2025-08-10T14:08:31.708347Z"}},"outputs":[{"name":"stdout","text":"----Nano model -----\n1.272656 million parameters\n----Small model -----\n5.079712 million parameters\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# sanity check\nx=torch.rand((1,3,640,640))\nout1,out2,out3=backbone_n(x)\nprint(out1.shape)\nprint(out2.shape)\nprint(out3.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:31.709700Z","iopub.execute_input":"2025-08-10T14:08:31.709987Z","iopub.status.idle":"2025-08-10T14:08:31.852700Z","shell.execute_reply.started":"2025-08-10T14:08:31.709959Z","shell.execute_reply":"2025-08-10T14:08:31.851975Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 64, 80, 80])\ntorch.Size([1, 128, 40, 40])\ntorch.Size([1, 256, 20, 20])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 2. Neck\nThe neck comprises of Upsample + C2f with \n\n**Upsample** = nearest-neighbor interpolation with scale_factor=2. It doesn't have trainable paramaters.","metadata":{}},{"cell_type":"code","source":"# upsample = nearest-neighbor interpolation with scale_factor=2\n#            doesn't have trainable paramaters\nclass Upsample(nn.Module):\n    def __init__(self,scale_factor=2,mode='nearest'):\n        super().__init__()\n        self.scale_factor=scale_factor\n        self.mode=mode\n\n    def forward(self,x):\n        return nn.functional.interpolate(x,scale_factor=self.scale_factor,mode=self.mode)\n    \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:31.853506Z","iopub.execute_input":"2025-08-10T14:08:31.853799Z","iopub.status.idle":"2025-08-10T14:08:31.858430Z","shell.execute_reply.started":"2025-08-10T14:08:31.853776Z","shell.execute_reply":"2025-08-10T14:08:31.857877Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Neck(nn.Module):\n    def __init__(self,version):\n        super().__init__()\n        d,w,r=yolo_params(version)\n\n        self.up=Upsample() # no trainable parameters\n        self.c2f_1=C2f(in_channels=int(512*w*(1+r)), out_channels=int(512*w),num_bottlenecks=int(3*d),shortcut=False)\n        self.c2f_2=C2f(in_channels=int(768*w), out_channels=int(256*w),num_bottlenecks=int(3*d),shortcut=False)\n        self.c2f_3=C2f(in_channels=int(768*w), out_channels=int(512*w),num_bottlenecks=int(3*d),shortcut=False)\n        self.c2f_4=C2f(in_channels=int(512*w*(1+r)), out_channels=int(512*w*r),num_bottlenecks=int(3*d),shortcut=False)\n\n        self.cv_1=Conv(in_channels=int(256*w),out_channels=int(256*w),kernel_size=3,stride=2, padding=1)\n        self.cv_2=Conv(in_channels=int(512*w),out_channels=int(512*w),kernel_size=3,stride=2, padding=1)\n\n\n    def forward(self,x_res_1,x_res_2,x):    \n        # x_res_1,x_res_2,x = output of backbone\n        res_1=x              # for residual connection\n        \n        x=self.up(x)\n        x=torch.cat([x,x_res_2],dim=1)\n\n        res_2=self.c2f_1(x)  # for residual connection\n        \n        x=self.up(res_2)\n        x=torch.cat([x,x_res_1],dim=1)\n\n        out_1=self.c2f_2(x)\n\n        x=self.cv_1(out_1)\n\n        x=torch.cat([x,res_2],dim=1)\n        out_2=self.c2f_3(x)\n\n        x=self.cv_2(out_2)\n\n        x=torch.cat([x,res_1],dim=1)\n        out_3=self.c2f_4(x)\n\n        return out_1,out_2,out_3\n    \n# sanity check\nneck=Neck(version='n')\nprint(f\"{sum(p.numel() for p in neck.parameters())/1e6} million parameters\")\n\nx=torch.rand((1,3,640,640))\nout1,out2,out3=Backbone(version='n')(x)\nout_1,out_2,out_3=neck(out1,out2,out3)\nprint(out_1.shape)\nprint(out_2.shape)\nprint(out_3.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:31.859131Z","iopub.execute_input":"2025-08-10T14:08:31.859585Z","iopub.status.idle":"2025-08-10T14:08:32.103069Z","shell.execute_reply.started":"2025-08-10T14:08:31.859566Z","shell.execute_reply":"2025-08-10T14:08:32.102255Z"}},"outputs":[{"name":"stdout","text":"0.98688 million parameters\ntorch.Size([1, 64, 80, 80])\ntorch.Size([1, 128, 40, 40])\ntorch.Size([1, 256, 20, 20])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### (a) DFL","metadata":{}},{"cell_type":"code","source":"# DFL\nclass DFL(nn.Module):\n    def __init__(self,ch=16):\n        super().__init__()\n        \n        self.ch=ch\n        \n        self.conv=nn.Conv2d(in_channels=ch,out_channels=1,kernel_size=1,bias=False).requires_grad_(False)\n        \n        # initialize conv with [0,...,ch-1]\n        x=torch.arange(ch,dtype=torch.float).view(1,ch,1,1)\n        self.conv.weight.data[:]=torch.nn.Parameter(x) # DFL only has ch parameters\n\n    def forward(self,x):\n        # x must have num_channels = 4*ch: x=[bs,4*ch,c]\n        b,c,a=x.shape                           # c=4*ch\n        x=x.view(b,4,self.ch,a).transpose(1,2)  # [bs,ch,4,a]\n\n        # take softmax on channel dimension to get distribution probabilities\n        x=x.softmax(1)                          # [b,ch,4,a]\n        x=self.conv(x)                          # [b,1,4,a]\n        return x.view(b,4,a)                    # [b,4,a]\n\n# sanity check\ndummy_input=torch.rand((1,64,128))\ndfl=DFL()\nprint(f\"{sum(p.numel() for p in dfl.parameters())} parameters\")\n\ndummy_output=dfl(dummy_input)\nprint(dummy_output.shape)\n\nprint(dfl)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:32.105329Z","iopub.execute_input":"2025-08-10T14:08:32.105603Z","iopub.status.idle":"2025-08-10T14:08:32.192852Z","shell.execute_reply.started":"2025-08-10T14:08:32.105584Z","shell.execute_reply":"2025-08-10T14:08:32.192157Z"}},"outputs":[{"name":"stdout","text":"16 parameters\ntorch.Size([1, 4, 128])\nDFL(\n  (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### (b) Head","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self,version,ch=16,num_classes=9):\n\n        super().__init__()\n        self.ch=ch                          # dfl channels\n        self.coordinates=self.ch*4          # number of bounding box coordinates \n        self.nc=num_classes                \n        self.no=self.coordinates+self.nc    # number of outputs per anchor box\n\n        self.stride=torch.zeros(3)          # strides computed during build\n        \n        d,w,r=yolo_params(version=version)\n        \n        # for bounding boxes\n        self.box=nn.ModuleList([\n            nn.Sequential(Conv(int(256*w),self.coordinates,kernel_size=3,stride=1,padding=1),\n                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w),self.coordinates,kernel_size=3,stride=1,padding=1),\n                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w*r),self.coordinates,kernel_size=3,stride=1,padding=1),\n                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1))\n        ])\n\n        # for classification\n        self.cls=nn.ModuleList([\n            nn.Sequential(Conv(int(256*w),self.nc,kernel_size=3,stride=1,padding=1),\n                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w),self.nc,kernel_size=3,stride=1,padding=1),\n                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w*r),self.nc,kernel_size=3,stride=1,padding=1),\n                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1))\n        ])\n\n        # dfl\n        self.dfl=DFL()\n\n    def forward(self,x):\n        # x = output of Neck = list of 3 tensors with different resolution and different channel dim\n        #     x[0]=[bs, ch0, w0, h0], x[1]=[bs, ch1, w1, h1], x[2]=[bs,ch2, w2, h2] \n\n        for i in range(len(self.box)):       # detection head i\n            box=self.box[i](x[i])            # [bs,num_coordinates,w,h]\n            cls=self.cls[i](x[i])            # [bs,num_classes,w,h]\n            x[i]=torch.cat((box,cls),dim=1)  # [bs,num_coordinates+num_classes,w,h]\n\n        # in training, no dfl output\n        if self.training:\n            return x                         # [3,bs,num_coordinates+num_classes,w,h]\n        \n        # in inference time, dfl produces refined bounding box coordinates\n        anchors, strides = (i.transpose(0, 1) for i in self.make_anchors(x, self.stride))\n\n        # concatenate predictions from all detection layers\n        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], dim=2) #[bs, 4*self.ch + self.nc, sum_i(h[i]w[i])]\n        \n        # split out predictions for box and cls\n        #           box=[bs,4×self.ch,sum_i(h[i]w[i])]\n        #           cls=[bs,self.nc,sum_i(h[i]w[i])]\n        box, cls = x.split(split_size=(4 * self.ch, self.nc), dim=1)\n\n\n        a, b = self.dfl(box).chunk(2, 1)  # a=b=[bs,2×self.ch,sum_i(h[i]w[i])]\n        a = anchors.unsqueeze(0) - a\n        b = anchors.unsqueeze(0) + b\n        box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n        \n        return torch.cat(tensors=(box * strides, cls.sigmoid()), dim=1)\n\n\n    def make_anchors(self, x, strides, offset=0.5):\n        # x= list of feature maps: x=[x[0],...,x[N-1]], in our case N= num_detection_heads=3\n        #                          each having shape [bs,ch,w,h]\n        #    each feature map x[i] gives output[i] = w*h anchor coordinates + w*h stride values\n        \n        # strides = list of stride values indicating how much \n        #           the spatial resolution of the feature map is reduced compared to the original image\n\n        assert x is not None\n        anchor_tensor, stride_tensor = [], []\n        dtype, device = x[0].dtype, x[0].device\n        for i, stride in enumerate(strides):\n            _, _, h, w = x[i].shape\n            sx = torch.arange(end=w, device=device, dtype=dtype) + offset  # x coordinates of anchor centers\n            sy = torch.arange(end=h, device=device, dtype=dtype) + offset  # y coordinates of anchor centers\n            sy, sx = torch.meshgrid(sy, sx)                                # all anchor centers \n            anchor_tensor.append(torch.stack((sx, sy), -1).view(-1, 2))\n            stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n        return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:32.193584Z","iopub.execute_input":"2025-08-10T14:08:32.193832Z","iopub.status.idle":"2025-08-10T14:08:32.209966Z","shell.execute_reply.started":"2025-08-10T14:08:32.193814Z","shell.execute_reply":"2025-08-10T14:08:32.209110Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\ndetect=Head(version='n')\nprint(f\"{sum(p.numel() for p in detect.parameters())/1e6} million parameters\")\n\n# out_1,out_2,out_3 are output of the neck\noutput=detect([out_1,out_2,out_3])\nprint(output[0].shape)\nprint(output[1].shape)\nprint(output[2].shape)\n\nprint(detect)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:32.210712Z","iopub.execute_input":"2025-08-10T14:08:32.210983Z","iopub.status.idle":"2025-08-10T14:08:32.296830Z","shell.execute_reply.started":"2025-08-10T14:08:32.210966Z","shell.execute_reply":"2025-08-10T14:08:32.296124Z"}},"outputs":[{"name":"stdout","text":"0.390118 million parameters\ntorch.Size([1, 66, 80, 80])\ntorch.Size([1, 66, 40, 40])\ntorch.Size([1, 66, 20, 20])\nHead(\n  (box): ModuleList(\n    (0): Sequential(\n      (0): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (1): Sequential(\n      (0): Conv(\n        (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (2): Sequential(\n      (0): Conv(\n        (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (cls): ModuleList(\n    (0): Sequential(\n      (0): Conv(\n        (conv): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (1): Sequential(\n      (0): Conv(\n        (conv): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (2): Sequential(\n      (0): Conv(\n        (conv): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (dfl): DFL(\n    (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  )\n)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 4. Putting everything together","metadata":{}},{"cell_type":"code","source":"class MyYolo(nn.Module):\n    def __init__(self,version):\n        super().__init__()\n        self.backbone=Backbone(version=version)\n        self.neck=Neck(version=version)\n        self.head=Head(version=version)\n\n    def forward(self,x):\n        x=self.backbone(x)              # return out1,out2,out3\n        x=self.neck(x[0],x[1],x[2])     # return out_1, out_2,out_3\n        return self.head(list(x))\n    \nmodel=MyYolo(version='n')\nprint(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:32.297537Z","iopub.execute_input":"2025-08-10T14:08:32.298058Z","iopub.status.idle":"2025-08-10T14:08:32.342322Z","shell.execute_reply.started":"2025-08-10T14:08:32.298032Z","shell.execute_reply":"2025-08-10T14:08:32.341738Z"}},"outputs":[{"name":"stdout","text":"2.649654 million parameters\nMyYolo(\n  (backbone): Backbone(\n    (conv_0): Conv(\n      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (conv_1): Conv(\n      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (conv_3): Conv(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (conv_5): Conv(\n      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (conv_7): Conv(\n      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (c2f_2): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (c2f_4): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0-1): 2 x Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (c2f_6): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0-1): 2 x Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (c2f_8): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (sppf): SPPF(\n      (conv1): Conv(\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (conv2): Conv(\n        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n    )\n  )\n  (neck): Neck(\n    (up): Upsample()\n    (c2f_1): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (c2f_2): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (c2f_3): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (c2f_4): C2f(\n      (conv1): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (conv1): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (conv2): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n      (conv2): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n    )\n    (cv_1): Conv(\n      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (cv_2): Conv(\n      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n  )\n  (head): Head(\n    (box): ModuleList(\n      (0): Sequential(\n        (0): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (1): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (1): Sequential(\n        (0): Conv(\n          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (1): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (2): Sequential(\n        (0): Conv(\n          (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (1): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (cls): ModuleList(\n      (0): Sequential(\n        (0): Conv(\n          (conv): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (1): Conv(\n          (conv): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (2): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (1): Sequential(\n        (0): Conv(\n          (conv): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (1): Conv(\n          (conv): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (2): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (2): Sequential(\n        (0): Conv(\n          (conv): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (1): Conv(\n          (conv): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn): BatchNorm2d(2, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (2): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (dfl): DFL(\n      (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# import shutil\n# import os\n\n# output_dir = \"/kaggle/working\"\n\n# # Delete everything inside the directory\n# for filename in os.listdir(output_dir):\n#     file_path = os.path.join(output_dir, filename)\n#     try:\n#         if os.path.isfile(file_path) or os.path.islink(file_path):\n#             os.unlink(file_path)  # remove file\n#         elif os.path.isdir(file_path):\n#             shutil.rmtree(file_path)  # remove foldear\n#     except Exception as e:\n#         print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n# print(\"/kaggle/working cleared\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:32.342998Z","iopub.execute_input":"2025-08-10T14:08:32.343182Z","iopub.status.idle":"2025-08-10T14:08:32.346582Z","shell.execute_reply.started":"2025-08-10T14:08:32.343168Z","shell.execute_reply":"2025-08-10T14:08:32.345824Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport shutil\nfrom pathlib import Path\n\n# Input dataset root (contains multiple class folders)\nsource_root = Path(\"/kaggle/input/urban-issues-dataset\")\n\n# Output YOLO dataset root\noutput_root = Path(\"/kaggle/working/dataset\")\nsplits = [\"train\", \"valid\", \"test\"]\n\n# Create YOLO folders\nfor split in splits:\n    (output_root / \"images\" / split).mkdir(parents=True, exist_ok=True)\n    (output_root / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n\n# Loop through each class folder\nfor class_folder in source_root.iterdir():\n    if class_folder.is_dir():\n        for split in splits:\n            img_dir = class_folder / class_folder.name / split / \"images\"\n            label_dir = class_folder / class_folder.name / split / \"labels\"\n\n            if not img_dir.exists() or not label_dir.exists():\n                continue  # skip if split doesn't exist for this class\n\n            for img_file in img_dir.glob(\"*.*\"):\n                # Copy image\n                dest_img_path = output_root / \"images\" / split / f\"{class_folder.name}_{img_file.name}\"\n                shutil.copy(img_file, dest_img_path)\n\n                # Copy label file (no ID remapping)\n                src_label_path = label_dir / f\"{img_file.stem}.txt\"\n                dest_label_path = output_root / \"labels\" / split / f\"{class_folder.name}_{img_file.stem}.txt\"\n                if src_label_path.exists():\n                    shutil.copy(src_label_path, dest_label_path)\n\nprint(\"YOLO dataset ready at:\", output_root)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:08:32.347379Z","iopub.execute_input":"2025-08-10T14:08:32.347612Z","iopub.status.idle":"2025-08-10T14:08:32.370506Z","shell.execute_reply.started":"2025-08-10T14:08:32.347598Z","shell.execute_reply":"2025-08-10T14:08:32.369934Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/yolo-model-training-files\")\n\nimport util\nfrom dataset import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:19:42.927844Z","iopub.execute_input":"2025-08-10T14:19:42.928408Z","iopub.status.idle":"2025-08-10T14:19:47.950270Z","shell.execute_reply.started":"2025-08-10T14:19:42.928386Z","shell.execute_reply":"2025-08-10T14:19:47.949656Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# train_fixed.py  (FINAL - with defaults)\nimport os\nimport sys\nimport yaml\nimport math\nimport shutil\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport cv2\nfrom PIL import Image\n\n# ----------------------\n# User-editable settings\n# ----------------------\nDATA_ROOT = os.environ.get(\"DATA_ROOT\", \"/kaggle/working/dataset\")  # change if needed\nTRAIN_IMG_SUB = os.path.join(\"images\", \"train\")\nVAL_IMG_SUB = os.path.join(\"images\", \"val\")\nLABELS_SUB = \"labels\"\nINPUT_SIZE = 640\nNUM_WORKERS = 0   # set to >0 after debug if desired\nBATCH_SIZE = 4    # lower if OOM\nEPOCHS = 4\nCHECKPOINT_DIR = \"weights\"\n\n# ----------------------\n# Fixed Dataset class (no augmentation)\n# ----------------------\nclass FixedDataset(torch.utils.data.Dataset):\n    FORMATS = ('bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp')\n\n    def __init__(self, filenames, input_size=640):\n        self.filenames = list(filenames)\n        self.input_size = input_size\n        # load labels (with cache handling)\n        self.labels = self._load_label(self.filenames)\n        self.indices = range(len(self.filenames))\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        img, (h0, w0) = self._load_image(img_path)\n        img_resized, ratio, pad = resize(img, self.input_size, augment=False)\n        # find label file path\n        label_path = self._label_path_from_image(img_path)\n        label = self.labels.get(img_path, np.zeros((0,5), dtype=np.float32)).copy()\n        # if label.size:\n        #     # after resize+pad, convert normalized xywh -> absolute xyxy, then later to normalized xywh relative to resized image\n        #     label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w0, ratio[1] * h0, pad[0], pad[1])\n        # else:\n        #     label = np.zeros((0,5), dtype=np.float32)\n\n        nl = len(label)\n        h_res, w_res = img_resized.shape[:2]\n        cls = label[:, 0:1] if nl else np.zeros((0,1), dtype=np.float32)\n        box = label[:, 1:5] if nl else np.zeros((0,4), dtype=np.float32)\n        box = xy2wh(box, w_res, h_res)  # normalized xywh w.r.t resized image\n\n        # make consistent: cls (n,1), box (n,4)\n        if nl:\n            target_cls = torch.from_numpy(cls).float().view(-1,1)\n            target_box = torch.from_numpy(box).float().view(-1,4)\n            idx_tensor = torch.zeros(nl, dtype=torch.long)\n        else:\n            target_cls = torch.zeros((0,1), dtype=torch.float32)\n            target_box = torch.zeros((0,4), dtype=torch.float32)\n            idx_tensor = torch.zeros((0,), dtype=torch.long)\n\n        # convert image to CHW RGB normalized tensor\n        img_rgb = img_resized[:, :, ::-1].transpose((2,0,1)).astype(np.float32) / 255.0\n        img_tensor = torch.from_numpy(np.ascontiguousarray(img_rgb))\n\n        return img_tensor, target_cls, target_box, idx_tensor\n\n    @staticmethod\n    def collate_fn(batch):\n        samples, cls_list, box_list, idx_list = zip(*batch)\n        imgs = torch.stack(samples, dim=0)\n\n        # normalize cls to (n,1)\n        cls_fixed = []\n        for c in cls_list:\n            if isinstance(c, np.ndarray):\n                c = torch.from_numpy(c)\n            if not isinstance(c, torch.Tensor):\n                c = torch.tensor(c)\n            if c.numel() == 0:\n                c = c.reshape(0,1)\n            elif c.dim() == 1:\n                c = c.view(-1,1)\n            else:\n                c = c.view(-1, c.shape[-1])\n            cls_fixed.append(c.float())\n\n        # normalize box to (n,4)\n        box_fixed = []\n        for b in box_list:\n            if isinstance(b, np.ndarray):\n                b = torch.from_numpy(b)\n            if not isinstance(b, torch.Tensor):\n                b = torch.tensor(b)\n            if b.numel() == 0:\n                b = b.reshape(0,4)\n            elif b.dim() == 1:\n                if b.numel() == 4:\n                    b = b.view(1,4)\n                else:\n                    b = b.view(-1,4)\n            else:\n                b = b.view(-1,4)\n            box_fixed.append(b.float())\n\n        cls = torch.cat(cls_fixed, dim=0) if sum(c.numel() for c in cls_fixed) > 0 else torch.zeros((0,1), dtype=torch.float32)\n        box = torch.cat(box_fixed, dim=0) if sum(b.numel() for b in box_fixed) > 0 else torch.zeros((0,4), dtype=torch.float32)\n\n        # build idx offsets\n        new_idx = []\n        for i, it in enumerate(idx_list):\n            if isinstance(it, np.ndarray):\n                it = torch.from_numpy(it)\n            if not isinstance(it, torch.Tensor):\n                it = torch.tensor(it)\n            if it.numel() == 0:\n                new_idx.append(it.reshape(0))\n            else:\n                new_idx.append((it.long() + i))\n        idx = torch.cat(new_idx, dim=0) if any(n.numel() for n in new_idx) else torch.zeros((0,), dtype=torch.long)\n\n        targets = {'cls': cls, 'box': box, 'idx': idx}\n        return imgs, targets\n\n    # -------------- helper I/O --------------\n    @staticmethod\n    def _label_path_from_image(img_path):\n        a = os.sep + \"images\" + os.sep\n        b = os.sep + \"labels\" + os.sep\n        if a in img_path:\n            return b.join(img_path.rsplit(a,1)).rsplit('.',1)[0] + '.txt'\n        else:\n            # fallback: replace images with labels in path\n            return img_path.replace(os.sep + \"images\" + os.sep, os.sep + \"labels\" + os.sep).rsplit('.',1)[0] + '.txt'\n\n    @staticmethod\n    def _load_image(path):\n        img = cv2.imread(path)\n        if img is None:\n            raise FileNotFoundError(path)\n        h,w = img.shape[:2]\n        return img, (h,w)\n\n    def _load_label(self, filenames):\n        cache_path = f\"{os.path.dirname(filenames[0])}.cache\"\n        # try load safe, otherwise rebuild\n        if os.path.exists(cache_path):\n            try:\n                data = torch.load(cache_path)\n                if isinstance(data, dict):\n                    return data\n                else:\n                    print(f\"[FixedDataset] cache {cache_path} invalid type; rebuilding.\")\n                    os.remove(cache_path)\n            except Exception as e:\n                print(f\"[FixedDataset] failed to load cache {cache_path}: {e}. Rebuilding.\")\n                try:\n                    os.remove(cache_path)\n                except:\n                    pass\n    \n        labels = {}\n        for img_path in filenames:\n            try:\n                with open(img_path, 'rb') as f:\n                    im = Image.open(f)\n                    im.verify()\n                shape = im.size\n                if not ((shape[0] > 9) and (shape[1] > 9)):\n                    # Skip small images (do not add to labels)\n                    continue\n                if im.format is None or im.format.lower() not in FixedDataset.FORMATS:\n                    # Skip unsupported formats\n                    continue\n            except Exception:\n                # Skip images that can't be opened or verified\n                continue\n    \n            # Construct label path by replacing /images/ with /labels/ and changing extension to .txt\n            img_dir, img_file = os.path.split(img_path)\n            label_dir = img_dir.replace('/images/', '/labels/')\n            label_file = os.path.splitext(img_file)[0] + '.txt'\n            label_path = os.path.join(label_dir, label_file)\n    \n            if os.path.isfile(label_path):\n                good = []\n                with open(label_path, 'r') as lf:\n                    for line in lf.read().strip().splitlines():\n                        parts = line.strip().split()\n                        if len(parts) >= 5:\n                            try:\n                                vals = [float(x) for x in parts[:5]]\n                                good.append(vals)\n                            except:\n                                continue\n                if len(good):\n                    arr = np.array(good, dtype=np.float32)\n                    if arr.shape[1] != 5:\n                        arr = arr[:, :5]\n                    arr[:,1:] = np.clip(arr[:,1:], 0.0, 1.0)\n                    labels[img_path] = arr\n                # else no labels -> skip this image (do not add to labels)\n            # else no label file -> skip this image (do not add to labels)\n    \n        try:\n            torch.save(labels, cache_path)\n        except Exception as e:\n            print(f\"[FixedDataset] warning: failed to write cache: {e}\")\n    \n        return labels\n     \n\n\n# ----------------------\n# small helper functions (resize, wh2xy, xy2wh)\n# ----------------------\ndef wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n    y = np.copy(x)\n    y[:,0] = w * (x[:,0] - x[:,2]/2) + pad_w\n    y[:,1] = h * (x[:,1] - x[:,3]/2) + pad_h\n    y[:,2] = w * (x[:,0] + x[:,2]/2) + pad_w\n    y[:,3] = h * (x[:,1] + x[:,3]/2) + pad_h\n    return y\n\ndef xy2wh(x, w, h):\n    if x.size == 0:\n        return x.reshape((0,4))\n    x[:,[0,2]] = x[:,[0,2]].clip(0, w - 1e-3)\n    x[:,[1,3]] = x[:,[1,3]].clip(0, h - 1e-3)\n    y = np.copy(x)\n    y[:,0] = ((x[:,0] + x[:,2]) / 2) / w\n    y[:,1] = ((x[:,1] + x[:,3]) / 2) / h\n    y[:,2] = (x[:,2] - x[:,0]) / w\n    y[:,3] = (x[:,3] - x[:,1]) / h\n    return y\n\ndef resample():\n    choices = (cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LINEAR,\n               cv2.INTER_NEAREST, cv2.INTER_LANCZOS4)\n    return np.random.choice(choices)\n\ndef resize(image, input_size, augment=False):\n    shape = image.shape[:2]  # h,w\n    r = min(input_size / shape[0], input_size / shape[1])\n    if not augment:\n        r = min(r, 1.0)\n    new_w = int(round(shape[1] * r))\n    new_h = int(round(shape[0] * r))\n    if (new_w, new_h) != (shape[1], shape[0]):\n        image = cv2.resize(image, (new_w, new_h), interpolation=resample())\n    pad_w = (input_size - new_w) / 2\n    pad_h = (input_size - new_h) / 2\n    top, bottom = int(round(pad_h - 0.1)), int(round(pad_h + 0.1))\n    left, right = int(round(pad_w - 0.1)), int(round(pad_w + 0.1))\n    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0,0,0))\n    return image, (r, r), (left, top)\n\n# ----------------------\n# Training entrypoint\n# ----------------------\ndef main():\n    # locate train images\n    train_img_dir = os.path.join(DATA_ROOT, TRAIN_IMG_SUB)\n    if not os.path.isdir(train_img_dir):\n        raise FileNotFoundError(f\"Train image folder not found: {train_img_dir}\")\n\n    img_exts = ('.jpg', '.jpeg', '.png', '.bmp')\n    filenames_train = [os.path.join(train_img_dir, f) for f in sorted(os.listdir(train_img_dir)) if f.lower().endswith(img_exts)]\n    if len(filenames_train) == 0:\n        raise RuntimeError(f\"No training images found in {train_img_dir}\")\n\n    print(f\"Found {len(filenames_train)} training images.\")\n\n    # load args.yaml (if exists)\n    params = {}\n    args_path = os.path.join(\"utils\", \"args.yaml\")\n    if os.path.isfile(args_path):\n        with open(args_path) as f:\n            params = yaml.safe_load(f)\n    else:\n        print(\"[train_fixed] Warning: utils/args.yaml not found, using defaults.\")\n\n    # --- SET DEFAULTS FOR KEYS REQUIRED BY ComputeLoss ---\n    defaults = {\n        'box': 7.5,\n        'cls': 0.5,\n        'dfl': 1.5,\n        'min_lr': 1e-4,\n        'max_lr': 1e-2,\n        'momentum': 0.937,\n        'weight_decay': 5e-4,\n        'warmup_epochs': 3.0,\n        'hsv_h': 0.015,\n        'hsv_s': 0.7,\n        'hsv_v': 0.4,\n        'degrees': 0.0,\n        'translate': 0.1,\n        'scale': 0.5,\n        'shear': 0.0,\n        'flip_ud': 0.0,\n        'flip_lr': 0.5,\n        'mosaic': 0.0,\n        'mix_up': 0.0,\n        'img_size': INPUT_SIZE,\n        'S': 20,\n        'B': 2,\n        'C': 2\n    }\n    for k, v in defaults.items():\n        params.setdefault(k, v)\n\n    # small summary print\n    print(\"Using params (sample):\")\n    for k in ['box', 'cls', 'dfl', 'lr', 'min_lr', 'max_lr', 'weight_decay', 'img_size']:\n        print(f\"  {k}: {params.get(k)}\")\n\n    batch_size = params.get(\"batch_size\", BATCH_SIZE)\n    epochs = params.get(\"epochs\", EPOCHS)\n    lr = params.get(\"lr\", 5e-4)\n\n    # dataset & loader\n    ds = FixedDataset(filenames_train, input_size=INPUT_SIZE)\n    loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=FixedDataset.collate_fn, pin_memory=True)\n    print(\"Train loader batches:\", len(loader))\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = MyYolo(version='n')  # adapt if your constructor differs\n    model = model.to(device)\n    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f} M\")\n\n    # pass params (now guaranteed to contain 'box','cls','dfl', etc.)\n    criterion = util.ComputeLoss(model, params)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=params.get(\"weight_decay\", 5e-4))\n\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n    # training loop\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_idx, (imgs, targets) in enumerate(loader):\n            imgs = imgs.float().to(device)\n            # move targets to device\n            targets = {k: v.to(device) for k,v in targets.items()}\n\n            outputs = model(imgs)  # use your model forward signature\n            losses = criterion(outputs, targets)  # returns tuple/list\n            loss = sum(losses)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            if (batch_idx + 1) % 20 == 0:\n                avg = running_loss / (batch_idx + 1)\n                print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{len(loader)} | avg_loss: {avg:.4f}\")\n\n        epoch_avg = running_loss / max(1, len(loader))\n        print(f\"Epoch {epoch+1} finished. Avg Loss: {epoch_avg:.4f}\")\n\n        # checkpoint\n        ckpt = {\"model\": model.state_dict(), \"epoch\": epoch+1, \"optimizer\": optimizer.state_dict()}\n        torch.save(ckpt, os.path.join(CHECKPOINT_DIR, f\"epoch{epoch+1}.pt\"))\n        torch.save(ckpt, os.path.join(CHECKPOINT_DIR, \"last.pt\"))\n\n    print(\"Training complete.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T14:42:56.517829Z","iopub.execute_input":"2025-08-10T14:42:56.518089Z","iopub.status.idle":"2025-08-10T14:51:48.730680Z","shell.execute_reply.started":"2025-08-10T14:42:56.518070Z","shell.execute_reply":"2025-08-10T14:51:48.730065Z"}},"outputs":[{"name":"stdout","text":"Found 5394 training images.\n[train_fixed] Warning: utils/args.yaml not found, using defaults.\nUsing params (sample):\n  box: 7.5\n  cls: 0.5\n  dfl: 1.5\n  lr: None\n  min_lr: 0.0001\n  max_lr: 0.01\n  weight_decay: 0.0005\n  img_size: 640\n[FixedDataset] failed to load cache /kaggle/working/dataset/images/train.cache: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.. Rebuilding.\nTrain loader batches: 1349\nModel parameters: 2.65 M\nEpoch 1/4 | Batch 20/1349 | avg_loss: 25859.7535\nEpoch 1/4 | Batch 40/1349 | avg_loss: 25524.3301\nEpoch 1/4 | Batch 60/1349 | avg_loss: 25199.1430\nEpoch 1/4 | Batch 80/1349 | avg_loss: 24942.9534\nEpoch 1/4 | Batch 100/1349 | avg_loss: 24733.2076\nEpoch 1/4 | Batch 120/1349 | avg_loss: 24532.0357\nEpoch 1/4 | Batch 140/1349 | avg_loss: 24341.9676\nEpoch 1/4 | Batch 160/1349 | avg_loss: 24156.0732\nEpoch 1/4 | Batch 180/1349 | avg_loss: 23978.5209\nEpoch 1/4 | Batch 200/1349 | avg_loss: 23804.8266\nEpoch 1/4 | Batch 220/1349 | avg_loss: 23633.1064\nEpoch 1/4 | Batch 240/1349 | avg_loss: 23464.2390\nEpoch 1/4 | Batch 260/1349 | avg_loss: 23298.6469\nEpoch 1/4 | Batch 280/1349 | avg_loss: 23137.0184\nEpoch 1/4 | Batch 300/1349 | avg_loss: 22973.5714\nEpoch 1/4 | Batch 320/1349 | avg_loss: 22809.9305\nEpoch 1/4 | Batch 340/1349 | avg_loss: 22646.9833\nEpoch 1/4 | Batch 360/1349 | avg_loss: 22483.7117\nEpoch 1/4 | Batch 380/1349 | avg_loss: 22319.6950\nEpoch 1/4 | Batch 400/1349 | avg_loss: 22155.6829\nEpoch 1/4 | Batch 420/1349 | avg_loss: 21989.9632\nEpoch 1/4 | Batch 440/1349 | avg_loss: 21822.4764\nEpoch 1/4 | Batch 460/1349 | avg_loss: 21654.6001\nEpoch 1/4 | Batch 480/1349 | avg_loss: 21486.5981\nEpoch 1/4 | Batch 500/1349 | avg_loss: 21318.5507\nEpoch 1/4 | Batch 520/1349 | avg_loss: 21150.5108\nEpoch 1/4 | Batch 540/1349 | avg_loss: 20983.0803\nEpoch 1/4 | Batch 560/1349 | avg_loss: 20816.2104\nEpoch 1/4 | Batch 580/1349 | avg_loss: 20650.0413\nEpoch 1/4 | Batch 600/1349 | avg_loss: 20483.8295\nEpoch 1/4 | Batch 620/1349 | avg_loss: 20317.9314\nEpoch 1/4 | Batch 640/1349 | avg_loss: 20152.5797\nEpoch 1/4 | Batch 660/1349 | avg_loss: 19987.8408\nEpoch 1/4 | Batch 680/1349 | avg_loss: 19823.4581\nEpoch 1/4 | Batch 700/1349 | avg_loss: 19659.8139\nEpoch 1/4 | Batch 720/1349 | avg_loss: 19496.6645\nEpoch 1/4 | Batch 740/1349 | avg_loss: 19334.4573\nEpoch 1/4 | Batch 760/1349 | avg_loss: 19173.2651\nEpoch 1/4 | Batch 780/1349 | avg_loss: 19012.8362\nEpoch 1/4 | Batch 800/1349 | avg_loss: 18853.3805\nEpoch 1/4 | Batch 820/1349 | avg_loss: 18695.8376\nEpoch 1/4 | Batch 840/1349 | avg_loss: 18538.5486\nEpoch 1/4 | Batch 860/1349 | avg_loss: 18382.3025\nEpoch 1/4 | Batch 880/1349 | avg_loss: 18227.1658\nEpoch 1/4 | Batch 900/1349 | avg_loss: 18073.0423\nEpoch 1/4 | Batch 920/1349 | avg_loss: 17920.1212\nEpoch 1/4 | Batch 940/1349 | avg_loss: 17768.4674\nEpoch 1/4 | Batch 960/1349 | avg_loss: 17618.0131\nEpoch 1/4 | Batch 980/1349 | avg_loss: 17468.7027\nEpoch 1/4 | Batch 1000/1349 | avg_loss: 17320.7270\nEpoch 1/4 | Batch 1020/1349 | avg_loss: 17174.0061\nEpoch 1/4 | Batch 1040/1349 | avg_loss: 17028.9434\nEpoch 1/4 | Batch 1060/1349 | avg_loss: 16885.7965\nEpoch 1/4 | Batch 1080/1349 | avg_loss: 16743.5812\nEpoch 1/4 | Batch 1100/1349 | avg_loss: 16602.2218\nEpoch 1/4 | Batch 1120/1349 | avg_loss: 16462.3457\nEpoch 1/4 | Batch 1140/1349 | avg_loss: 16323.7897\nEpoch 1/4 | Batch 1160/1349 | avg_loss: 16186.5142\nEpoch 1/4 | Batch 1180/1349 | avg_loss: 16050.8271\nEpoch 1/4 | Batch 1200/1349 | avg_loss: 15916.4578\nEpoch 1/4 | Batch 1220/1349 | avg_loss: 15783.5130\nEpoch 1/4 | Batch 1240/1349 | avg_loss: 15651.9414\nEpoch 1/4 | Batch 1260/1349 | avg_loss: 15521.7263\nEpoch 1/4 | Batch 1280/1349 | avg_loss: 15393.2147\nEpoch 1/4 | Batch 1300/1349 | avg_loss: 15266.1136\nEpoch 1/4 | Batch 1320/1349 | avg_loss: 15140.3172\nEpoch 1/4 | Batch 1340/1349 | avg_loss: 15015.9613\nEpoch 1 finished. Avg Loss: 14958.0121\nEpoch 2/4 | Batch 20/1349 | avg_loss: 6594.8727\nEpoch 2/4 | Batch 40/1349 | avg_loss: 6518.6694\nEpoch 2/4 | Batch 60/1349 | avg_loss: 6446.4368\nEpoch 2/4 | Batch 80/1349 | avg_loss: 6374.4703\nEpoch 2/4 | Batch 100/1349 | avg_loss: 6304.7672\nEpoch 2/4 | Batch 120/1349 | avg_loss: 6235.3087\nEpoch 2/4 | Batch 140/1349 | avg_loss: 6167.5891\nEpoch 2/4 | Batch 160/1349 | avg_loss: 6101.8297\nEpoch 2/4 | Batch 180/1349 | avg_loss: 6037.7165\nEpoch 2/4 | Batch 200/1349 | avg_loss: 5974.1073\nEpoch 2/4 | Batch 220/1349 | avg_loss: 5911.7641\nEpoch 2/4 | Batch 240/1349 | avg_loss: 5849.6734\nEpoch 2/4 | Batch 260/1349 | avg_loss: 5788.9752\nEpoch 2/4 | Batch 280/1349 | avg_loss: 5729.5294\nEpoch 2/4 | Batch 300/1349 | avg_loss: 5671.1623\nEpoch 2/4 | Batch 320/1349 | avg_loss: 5614.1897\nEpoch 2/4 | Batch 340/1349 | avg_loss: 5557.5388\nEpoch 2/4 | Batch 360/1349 | avg_loss: 5502.5163\nEpoch 2/4 | Batch 380/1349 | avg_loss: 5448.1221\nEpoch 2/4 | Batch 400/1349 | avg_loss: 5393.9256\nEpoch 2/4 | Batch 420/1349 | avg_loss: 5340.7375\nEpoch 2/4 | Batch 440/1349 | avg_loss: 5288.2737\nEpoch 2/4 | Batch 460/1349 | avg_loss: 5237.0187\nEpoch 2/4 | Batch 480/1349 | avg_loss: 5186.3621\nEpoch 2/4 | Batch 500/1349 | avg_loss: 5136.6165\nEpoch 2/4 | Batch 520/1349 | avg_loss: 5087.6887\nEpoch 2/4 | Batch 540/1349 | avg_loss: 5039.9890\nEpoch 2/4 | Batch 560/1349 | avg_loss: 4992.8874\nEpoch 2/4 | Batch 580/1349 | avg_loss: 4946.3560\nEpoch 2/4 | Batch 600/1349 | avg_loss: 4900.5539\nEpoch 2/4 | Batch 620/1349 | avg_loss: 4855.5378\nEpoch 2/4 | Batch 640/1349 | avg_loss: 4811.2518\nEpoch 2/4 | Batch 660/1349 | avg_loss: 4767.5588\nEpoch 2/4 | Batch 680/1349 | avg_loss: 4724.5832\nEpoch 2/4 | Batch 700/1349 | avg_loss: 4682.3326\nEpoch 2/4 | Batch 720/1349 | avg_loss: 4640.6702\nEpoch 2/4 | Batch 740/1349 | avg_loss: 4599.5805\nEpoch 2/4 | Batch 760/1349 | avg_loss: 4559.0436\nEpoch 2/4 | Batch 780/1349 | avg_loss: 4519.2373\nEpoch 2/4 | Batch 800/1349 | avg_loss: 4480.0487\nEpoch 2/4 | Batch 820/1349 | avg_loss: 4441.5194\nEpoch 2/4 | Batch 840/1349 | avg_loss: 4403.4855\nEpoch 2/4 | Batch 860/1349 | avg_loss: 4366.3597\nEpoch 2/4 | Batch 880/1349 | avg_loss: 4329.6638\nEpoch 2/4 | Batch 900/1349 | avg_loss: 4293.3422\nEpoch 2/4 | Batch 920/1349 | avg_loss: 4257.4421\nEpoch 2/4 | Batch 940/1349 | avg_loss: 4222.1895\nEpoch 2/4 | Batch 960/1349 | avg_loss: 4187.4996\nEpoch 2/4 | Batch 980/1349 | avg_loss: 4153.2019\nEpoch 2/4 | Batch 1000/1349 | avg_loss: 4119.4697\nEpoch 2/4 | Batch 1020/1349 | avg_loss: 4086.3360\nEpoch 2/4 | Batch 1040/1349 | avg_loss: 4053.5480\nEpoch 2/4 | Batch 1060/1349 | avg_loss: 4021.1563\nEpoch 2/4 | Batch 1080/1349 | avg_loss: 3989.2525\nEpoch 2/4 | Batch 1100/1349 | avg_loss: 3957.7371\nEpoch 2/4 | Batch 1120/1349 | avg_loss: 3926.6831\nEpoch 2/4 | Batch 1140/1349 | avg_loss: 3896.0334\nEpoch 2/4 | Batch 1160/1349 | avg_loss: 3865.8675\nEpoch 2/4 | Batch 1180/1349 | avg_loss: 3836.1138\nEpoch 2/4 | Batch 1200/1349 | avg_loss: 3806.7440\nEpoch 2/4 | Batch 1220/1349 | avg_loss: 3777.7783\nEpoch 2/4 | Batch 1240/1349 | avg_loss: 3749.1367\nEpoch 2/4 | Batch 1260/1349 | avg_loss: 3720.8833\nEpoch 2/4 | Batch 1280/1349 | avg_loss: 3692.9697\nEpoch 2/4 | Batch 1300/1349 | avg_loss: 3665.4749\nEpoch 2/4 | Batch 1320/1349 | avg_loss: 3638.4060\nEpoch 2/4 | Batch 1340/1349 | avg_loss: 3611.6449\nEpoch 2 finished. Avg Loss: 3599.0148\nEpoch 3/4 | Batch 20/1349 | avg_loss: 1798.9892\nEpoch 3/4 | Batch 40/1349 | avg_loss: 1784.5756\nEpoch 3/4 | Batch 60/1349 | avg_loss: 1771.2440\nEpoch 3/4 | Batch 80/1349 | avg_loss: 1759.0607\nEpoch 3/4 | Batch 100/1349 | avg_loss: 1745.7051\nEpoch 3/4 | Batch 120/1349 | avg_loss: 1732.1092\nEpoch 3/4 | Batch 140/1349 | avg_loss: 1718.1603\nEpoch 3/4 | Batch 160/1349 | avg_loss: 1706.8892\nEpoch 3/4 | Batch 180/1349 | avg_loss: 1695.1676\nEpoch 3/4 | Batch 200/1349 | avg_loss: 1683.3946\nEpoch 3/4 | Batch 220/1349 | avg_loss: 1670.8550\nEpoch 3/4 | Batch 240/1349 | avg_loss: 1658.2754\nEpoch 3/4 | Batch 260/1349 | avg_loss: 1645.8652\nEpoch 3/4 | Batch 280/1349 | avg_loss: 1633.4578\nEpoch 3/4 | Batch 300/1349 | avg_loss: 1620.9601\nEpoch 3/4 | Batch 320/1349 | avg_loss: 1608.3711\nEpoch 3/4 | Batch 340/1349 | avg_loss: 1596.1414\nEpoch 3/4 | Batch 360/1349 | avg_loss: 1584.2429\nEpoch 3/4 | Batch 380/1349 | avg_loss: 1572.6642\nEpoch 3/4 | Batch 400/1349 | avg_loss: 1560.8195\nEpoch 3/4 | Batch 420/1349 | avg_loss: 1549.1775\nEpoch 3/4 | Batch 440/1349 | avg_loss: 1537.8782\nEpoch 3/4 | Batch 460/1349 | avg_loss: 1526.6388\nEpoch 3/4 | Batch 480/1349 | avg_loss: 1515.3783\nEpoch 3/4 | Batch 500/1349 | avg_loss: 1504.2373\nEpoch 3/4 | Batch 520/1349 | avg_loss: 1493.3169\nEpoch 3/4 | Batch 540/1349 | avg_loss: 1482.5823\nEpoch 3/4 | Batch 560/1349 | avg_loss: 1471.9417\nEpoch 3/4 | Batch 580/1349 | avg_loss: 1461.5705\nEpoch 3/4 | Batch 600/1349 | avg_loss: 1451.3281\nEpoch 3/4 | Batch 620/1349 | avg_loss: 1441.3678\nEpoch 3/4 | Batch 640/1349 | avg_loss: 1431.3323\nEpoch 3/4 | Batch 660/1349 | avg_loss: 1421.4093\nEpoch 3/4 | Batch 680/1349 | avg_loss: 1411.4621\nEpoch 3/4 | Batch 700/1349 | avg_loss: 1401.6188\nEpoch 3/4 | Batch 720/1349 | avg_loss: 1391.9150\nEpoch 3/4 | Batch 740/1349 | avg_loss: 1382.3478\nEpoch 3/4 | Batch 760/1349 | avg_loss: 1373.1143\nEpoch 3/4 | Batch 780/1349 | avg_loss: 1363.7127\nEpoch 3/4 | Batch 800/1349 | avg_loss: 1354.5223\nEpoch 3/4 | Batch 820/1349 | avg_loss: 1345.3923\nEpoch 3/4 | Batch 840/1349 | avg_loss: 1336.3184\nEpoch 3/4 | Batch 860/1349 | avg_loss: 1327.3457\nEpoch 3/4 | Batch 880/1349 | avg_loss: 1318.4854\nEpoch 3/4 | Batch 900/1349 | avg_loss: 1309.7793\nEpoch 3/4 | Batch 920/1349 | avg_loss: 1301.2578\nEpoch 3/4 | Batch 940/1349 | avg_loss: 1292.7455\nEpoch 3/4 | Batch 960/1349 | avg_loss: 1284.4600\nEpoch 3/4 | Batch 980/1349 | avg_loss: 1276.1217\nEpoch 3/4 | Batch 1000/1349 | avg_loss: 1267.8961\nEpoch 3/4 | Batch 1020/1349 | avg_loss: 1259.7075\nEpoch 3/4 | Batch 1040/1349 | avg_loss: 1251.5906\nEpoch 3/4 | Batch 1060/1349 | avg_loss: 1243.6780\nEpoch 3/4 | Batch 1080/1349 | avg_loss: 1235.7189\nEpoch 3/4 | Batch 1100/1349 | avg_loss: 1227.8300\nEpoch 3/4 | Batch 1120/1349 | avg_loss: 1220.1281\nEpoch 3/4 | Batch 1140/1349 | avg_loss: 1212.4042\nEpoch 3/4 | Batch 1160/1349 | avg_loss: 1204.7575\nEpoch 3/4 | Batch 1180/1349 | avg_loss: 1197.2343\nEpoch 3/4 | Batch 1200/1349 | avg_loss: 1189.8292\nEpoch 3/4 | Batch 1220/1349 | avg_loss: 1182.4353\nEpoch 3/4 | Batch 1240/1349 | avg_loss: 1175.0828\nEpoch 3/4 | Batch 1260/1349 | avg_loss: 1167.8028\nEpoch 3/4 | Batch 1280/1349 | avg_loss: 1160.6335\nEpoch 3/4 | Batch 1300/1349 | avg_loss: 1153.5344\nEpoch 3/4 | Batch 1320/1349 | avg_loss: 1146.5538\nEpoch 3/4 | Batch 1340/1349 | avg_loss: 1139.5774\nEpoch 3 finished. Avg Loss: 1136.2026\nEpoch 4/4 | Batch 20/1349 | avg_loss: 667.2367\nEpoch 4/4 | Batch 40/1349 | avg_loss: 662.5536\nEpoch 4/4 | Batch 60/1349 | avg_loss: 658.4446\nEpoch 4/4 | Batch 80/1349 | avg_loss: 654.0290\nEpoch 4/4 | Batch 100/1349 | avg_loss: 651.4485\nEpoch 4/4 | Batch 120/1349 | avg_loss: 648.5195\nEpoch 4/4 | Batch 140/1349 | avg_loss: 643.4430\nEpoch 4/4 | Batch 160/1349 | avg_loss: 638.7774\nEpoch 4/4 | Batch 180/1349 | avg_loss: 634.3054\nEpoch 4/4 | Batch 200/1349 | avg_loss: 630.2736\nEpoch 4/4 | Batch 220/1349 | avg_loss: 626.0115\nEpoch 4/4 | Batch 240/1349 | avg_loss: 621.7448\nEpoch 4/4 | Batch 260/1349 | avg_loss: 617.9051\nEpoch 4/4 | Batch 280/1349 | avg_loss: 614.1169\nEpoch 4/4 | Batch 300/1349 | avg_loss: 610.2168\nEpoch 4/4 | Batch 320/1349 | avg_loss: 606.2208\nEpoch 4/4 | Batch 340/1349 | avg_loss: 602.2331\nEpoch 4/4 | Batch 360/1349 | avg_loss: 598.2528\nEpoch 4/4 | Batch 380/1349 | avg_loss: 594.4796\nEpoch 4/4 | Batch 400/1349 | avg_loss: 590.6429\nEpoch 4/4 | Batch 420/1349 | avg_loss: 586.8058\nEpoch 4/4 | Batch 440/1349 | avg_loss: 583.1251\nEpoch 4/4 | Batch 460/1349 | avg_loss: 579.4185\nEpoch 4/4 | Batch 480/1349 | avg_loss: 575.6989\nEpoch 4/4 | Batch 500/1349 | avg_loss: 572.0985\nEpoch 4/4 | Batch 520/1349 | avg_loss: 569.9521\nEpoch 4/4 | Batch 540/1349 | avg_loss: 567.0085\nEpoch 4/4 | Batch 560/1349 | avg_loss: 563.6542\nEpoch 4/4 | Batch 580/1349 | avg_loss: 560.2289\nEpoch 4/4 | Batch 600/1349 | avg_loss: 556.9877\nEpoch 4/4 | Batch 620/1349 | avg_loss: 553.6555\nEpoch 4/4 | Batch 640/1349 | avg_loss: 550.3276\nEpoch 4/4 | Batch 660/1349 | avg_loss: 547.0488\nEpoch 4/4 | Batch 680/1349 | avg_loss: 543.7557\nEpoch 4/4 | Batch 700/1349 | avg_loss: 540.4656\nEpoch 4/4 | Batch 720/1349 | avg_loss: 537.1823\nEpoch 4/4 | Batch 740/1349 | avg_loss: 533.9337\nEpoch 4/4 | Batch 760/1349 | avg_loss: 530.7493\nEpoch 4/4 | Batch 780/1349 | avg_loss: 527.5449\nEpoch 4/4 | Batch 800/1349 | avg_loss: 524.3919\nEpoch 4/4 | Batch 820/1349 | avg_loss: 521.4240\nEpoch 4/4 | Batch 840/1349 | avg_loss: 518.3231\nEpoch 4/4 | Batch 860/1349 | avg_loss: 515.4108\nEpoch 4/4 | Batch 880/1349 | avg_loss: 512.5822\nEpoch 4/4 | Batch 900/1349 | avg_loss: 509.6653\nEpoch 4/4 | Batch 920/1349 | avg_loss: 506.7252\nEpoch 4/4 | Batch 940/1349 | avg_loss: 503.7924\nEpoch 4/4 | Batch 960/1349 | avg_loss: 500.8406\nEpoch 4/4 | Batch 980/1349 | avg_loss: 497.9348\nEpoch 4/4 | Batch 1000/1349 | avg_loss: 495.0484\nEpoch 4/4 | Batch 1020/1349 | avg_loss: 492.1314\nEpoch 4/4 | Batch 1040/1349 | avg_loss: 489.2737\nEpoch 4/4 | Batch 1060/1349 | avg_loss: 486.4221\nEpoch 4/4 | Batch 1080/1349 | avg_loss: 483.6095\nEpoch 4/4 | Batch 1100/1349 | avg_loss: 480.7986\nEpoch 4/4 | Batch 1120/1349 | avg_loss: 478.0161\nEpoch 4/4 | Batch 1140/1349 | avg_loss: 475.2397\nEpoch 4/4 | Batch 1160/1349 | avg_loss: 472.4970\nEpoch 4/4 | Batch 1180/1349 | avg_loss: 469.7803\nEpoch 4/4 | Batch 1200/1349 | avg_loss: 467.0974\nEpoch 4/4 | Batch 1220/1349 | avg_loss: 464.4394\nEpoch 4/4 | Batch 1240/1349 | avg_loss: 461.8235\nEpoch 4/4 | Batch 1260/1349 | avg_loss: 459.2277\nEpoch 4/4 | Batch 1280/1349 | avg_loss: 456.6761\nEpoch 4/4 | Batch 1300/1349 | avg_loss: 454.1369\nEpoch 4/4 | Batch 1320/1349 | avg_loss: 451.6305\nEpoch 4/4 | Batch 1340/1349 | avg_loss: 449.1351\nEpoch 4 finished. Avg Loss: 447.9150\nTraining complete.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}